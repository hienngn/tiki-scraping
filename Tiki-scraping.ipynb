{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "from time import sleep\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url(url):\n",
    "    \"\"\"Get parsed HTML from url\n",
    "      Input: url to the webpage\n",
    "      Output: Parsed HTML text of the webpage\n",
    "    \"\"\"\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "    try: \n",
    "        # Send GET request\n",
    "        r = requests.get(url, headers=headers)\n",
    "\n",
    "        # Parse HTML text\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "        return soup\n",
    "    \n",
    "    except:\n",
    "        print('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create database\n",
    "\n",
    "conn = sqlite3.connect('tiki.db')\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Category urls**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Category table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create \"categories\" table\n",
    "def create_categories_table():\n",
    "    query = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS categories (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            category VARCHAR(255),\n",
    "            category_url TEXT,\n",
    "            parent_category VARCHAR(255)\n",
    "        )\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cur.execute(query)\n",
    "        conn.commit()\n",
    "    except Exception as err:\n",
    "        print('ERROR BY CREATE TABLE', err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_category_table(row):\n",
    "    query = \"\"\"\n",
    "    INSERT INTO categories (category, category_url, parent_category)\n",
    "    VALUES (?, ?, ?);\n",
    "    \"\"\"\n",
    "    val = (i['category'], i['category_url'], i['parent_category'])\n",
    "    try:\n",
    "        cur.execute(query, val)\n",
    "        cat_id = cur.lastrowid\n",
    "        print(cat_id)\n",
    "        conn.commit()\n",
    "\n",
    "    except Exception as err:\n",
    "        print('ERROR BY INSERT:', err)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping category URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get URL of 16 main categories:\n",
    "def get_main_categories(url):\n",
    "    mainsoup = get_url(url)\n",
    "    regex = re.compile('.*MenuItem-.*')\n",
    "    try:\n",
    "        cat_links = mainsoup.find_all('li', {'class':regex})\n",
    "        data = []\n",
    "\n",
    "        for i in cat_links:\n",
    "            d = {'category':'', 'category_url':'', 'parent_category':''}\n",
    "            d['category_url'] = i.a['href'].split('?')[0]\n",
    "            d['category'] = i.find('span', {'class':'text'}).get_text()\n",
    "            d['parent_category'] = ''\n",
    "            data.append(d)\n",
    "\n",
    "        return data\n",
    "\n",
    "    except:\n",
    "        print('ERROR BY GET MAIN CATEGORIES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get URL of sub categories\n",
    "def get_sub_categories(parent_category, url):\n",
    "    subsoup = get_url(url)\n",
    "\n",
    "    data = []\n",
    "    try:\n",
    "        sub_cats = subsoup.find_all('div', {'class':'list-group-item is-child'})\n",
    "\n",
    "        for eachdiv in sub_cats:\n",
    "            d = {'category':'', 'category_url':'', 'parent_category':''}\n",
    "            \n",
    "            geturl = eachdiv.find('a', {'class':'list-group-item'})\n",
    "            d['category_url'] = tikiurl + geturl['href']\n",
    "            \n",
    "            gettitle = geturl.get_text()\n",
    "            d['category']  = re.sub('\\s{2,}', ' ', gettitle).strip(\"\\n\")\n",
    "            \n",
    "            d['parent_category'] = parent_category\n",
    "            \n",
    "            # Add result to category table\n",
    "            add_to_category_table(d)\n",
    "            \n",
    "            data.append(d)\n",
    "        return data\n",
    "    except:\n",
    "        print('ERROR BY GET SUB CATEGORIES')\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_categories(categories, sumlist):\n",
    "    if len(categories) == 0: # end condition\n",
    "        return []\n",
    "    else:\n",
    "        for cat in categories:\n",
    "            sub_categories = get_sub_categories(cat['category'], cat['category_url'])\n",
    "            sumlist.extend(sub_categories)\n",
    "            if len(sumlist) % 100 == 0:\n",
    "                print(len(sumlist))\n",
    "#             sleep(1)\n",
    "            get_all_categories(sub_categories, sumlist)\n",
    "        return sumlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End-to-end scraping all categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_all_categories(url):\n",
    "    \n",
    "    # Create database\n",
    "    conn = sqlite3.connect('tiki.db')\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    # Create category table\n",
    "    create_categories_table()\n",
    "    \n",
    "    #Scrape 16 main categories\n",
    "    main_categories = get_main_categories(url)\n",
    "    print('Done scraping main categories, total: ', len(main_categories))\n",
    "    \n",
    "    # Scrape all sub categories\n",
    "    try:\n",
    "        all_categories = get_all_categories(main_categories, [])\n",
    "        return all_categories\n",
    "    \n",
    "    except:\n",
    "        print('Error')\n",
    "        return all_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Product information**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Product table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    " def create_products_table():\n",
    "    query = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS products (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            cat_id INTEGER,\n",
    "            product_category VARCHAR(255),\n",
    "            product_brand VARCHAR(255),\n",
    "            product_title VARCHAR(255),\n",
    "            price VARCHAR(255),\n",
    "            sku INTEGER,\n",
    "            image_url TEXT,\n",
    "            sale INTEGER,\n",
    "            review INTEGER,\n",
    "            rating INTEGER,\n",
    "            product_url TEXT,\n",
    "            FOREIGN KEY (cat_id) REFERENCES categories(id)\n",
    "        )\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cur.execute(query)\n",
    "        conn.commit()\n",
    "    except Exception as err:\n",
    "        print('ERROR BY CREATE PRODUCTS TABLE', err)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Product:\n",
    "    def __init__(self, cat_id, category, brand, title, price, sku, image, sale, review, rating, url):\n",
    "        self.cat_id = cat_id\n",
    "        self.category = category\n",
    "        self.brand = brand\n",
    "        self.title = title\n",
    "        self.price = price\n",
    "        self.sku = sku\n",
    "        self.image = image\n",
    "        self.sale = sale\n",
    "        self.review = review\n",
    "        self.rating = rating\n",
    "        self.url = url\n",
    "\n",
    "    def save_into_db(self):\n",
    "        query = \"\"\"\n",
    "            INSERT INTO products (cat_id,\n",
    "            product_category,\n",
    "            product_brand,\n",
    "            product_title,\n",
    "            price,\n",
    "            sku,\n",
    "            image_url,\n",
    "            sale,\n",
    "            review,\n",
    "            rating,\n",
    "            product_url)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?);\n",
    "        \"\"\"\n",
    "        val = (self.cat_id, self.category, self.brand, self.title, self.price, self.sku, self.image, self.sale,\n",
    "               self.review, self.rating, self.url)\n",
    "        try:\n",
    "            cur.execute(query, val)\n",
    "            conn.commit()\n",
    "        except Exception as err:\n",
    "            print('ERROR BY INSERT:', err)\n",
    "\n",
    "# cat1 = Category('Test 3', 'Test URL 3')\n",
    "# cat1.save_into_db()\n",
    "# print(cat1.cat_id)\n",
    "# cur.execute('SELECT * FROM categories;').fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping product information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sale(div): \n",
    "    getsale = div.find('span', {'class':'sale-tag'})\n",
    "    sale = int(getsale.get_text()[1:-1]) if getsale else \"No discount\"\n",
    "\n",
    "    return sale\n",
    "\n",
    "def get_img_url(div):\n",
    "    image = div.img['src']\n",
    "    \n",
    "    return image\n",
    "\n",
    "def get_rating(div):\n",
    "    getrating = div.find('span', {'class':'rating-content'})\n",
    "    rating = int(getrating.span['style'][6:-1])/20 if getrating else \"No rating\"\n",
    "    \n",
    "    return rating\n",
    "\n",
    "def get_review(div):\n",
    "    getreview = div.find('p', {'class': 'review'})\n",
    "    reviews = getreview.get_text().strip('()')\n",
    "    if reviews[:-9] == 'Chưa có':\n",
    "        review = 0\n",
    "    else:\n",
    "        review = int(reviews[:-9])\n",
    "        \n",
    "    return review\n",
    "\n",
    "def get_url(div):\n",
    "    getlink = div.a['href']\n",
    "    if getlink[:11] == \"//t.ants.vn\":\n",
    "        url = \"http:\" + getlink\n",
    "    else:\n",
    "        url = tikiurl + getlink    \n",
    "    \n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_category(catid, category, categoryurl, save_db=False):\n",
    "    '''Scrape one category from tiki.vn\n",
    "        Input: one link of category (removed \"?\" and the part after it)\n",
    "        Output: dataframe contains details of products\n",
    "    '''\n",
    "    products = []\n",
    "    \n",
    "\n",
    "    for i in range(1, 10):\n",
    "        sleep(3)\n",
    "        \n",
    "        try:\n",
    "    \n",
    "            soup = get_url(categoryurl + f'?page={i}') # apply function\n",
    "\n",
    "            product_div = soup.find_all('div', {'class':'product-item'})\n",
    "\n",
    "            if len(product_div) == 0:\n",
    "                break\n",
    "\n",
    "            for prod in product_div:\n",
    "\n",
    "\n",
    "                #first note:\n",
    "                category= prod['data-category']\n",
    "                brand = prod['data-brand']\n",
    "                title = prod['data-title']\n",
    "                price = prod['data-price']\n",
    "                sku = prod['product-sku']\n",
    "                cat_id = catid\n",
    "\n",
    "                # lower levels\n",
    "\n",
    "                sale = get_sale(prod)\n",
    "\n",
    "                # image URL\n",
    "                image = get_img_url(prod)\n",
    "\n",
    "                # Rating. Can have no rating\n",
    "                rating = get_rating(prod)\n",
    "\n",
    "                #Review\n",
    "                reviews = get_review(prod)\n",
    "\n",
    "                # Product URL\n",
    "                url = get_url(prod)\n",
    "\n",
    "                prod = Product(cat_id, category, brand, title, price, sku, image, sale, review, rating, url)\n",
    "                if save_db:\n",
    "                    prod.save_into_db()\n",
    "                products.append(prod)\n",
    "        except Exception as err:\n",
    "            print('ERROR BY GET SUB CATEGORIES:', err)\n",
    "    \n",
    "    return products\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_layer = pd.read_sql_query('''SELECT A.id, A.category, A.category_url, B.parent_category\n",
    "FROM categories as A LEFT JOIN categories AS B on A.category = B.parent_category\n",
    "WHERE B.parent_category IS NULL;''', conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_products = []\n",
    "\n",
    "for index, row in last_layer.iterrows():\n",
    "    d = scrape_category(row['id'],row['category'], row['category_url'], save_db=True)\n",
    "    all_products.append(d)\n",
    "    if len(all_products) % 1000 == 0: \n",
    "        print('Successfully scraped: ', len(all_products))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
